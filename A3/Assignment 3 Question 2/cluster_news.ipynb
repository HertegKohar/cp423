{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "import re\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import (\n",
    "    KMeans,\n",
    "    AgglomerativeClustering,\n",
    "    DBSCAN,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    adjusted_mutual_info_score,\n",
    "    adjusted_rand_score,\n",
    "    completeness_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF parameters\n",
    "MAX_FEATURES = 500\n",
    "MIN_DF = 50\n",
    "MAX_DF = 500\n",
    "\n",
    "MODEL_PATH = \"models\\\\\"\n",
    "\n",
    "def concatenate_data():\n",
    "    data = []\n",
    "    columns = [\"news_group\", \"article_number\", \"cluster\", \"text\"]\n",
    "    path = \"20_newsgroups\"\n",
    "    news_group_index = -1\n",
    "    for news_group in os.listdir(path):\n",
    "        news_group_index += 1\n",
    "        # append all articles in news group to data\n",
    "        for article in os.listdir(os.path.join(path, news_group)):\n",
    "            with open(os.path.join(path, news_group, article), \"r\") as f:\n",
    "                text = f.read()\n",
    "            data.append([news_group_index, article, -1, text])\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df[\"news_group\"] = pd.to_numeric(df[\"news_group\"])\n",
    "    df[\"article_number\"] = pd.to_numeric(df[\"article_number\"])\n",
    "    df[\"cluster\"] = pd.to_numeric(df[\"cluster\"])\n",
    "    return df\n",
    "\n",
    "def preprocess(text_data):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # remove headers\n",
    "    text_data = text_data.apply(\n",
    "        lambda x: re.sub(r'^.*?\\n\\n', '', x, flags=re.DOTALL)\n",
    "    )\n",
    "    # tokenize and lowercase\n",
    "    text_data = text_data.apply(\n",
    "        lambda x: tokenizer.tokenize(x.lower())\n",
    "    )\n",
    "    # remove stop words\n",
    "    text_data = text_data.apply(\n",
    "        lambda x: [word for word in x if word not in stop_words]\n",
    "    )\n",
    "    text_data = text_data.apply(lambda x: \" \".join(x))\n",
    "    return text_data\n",
    "\n",
    "def load_or_create_preprocessed():\n",
    "    print(\"Loading or creating preprocessed data...\")\n",
    "    # use preprocessed data file if it exists, else create it\n",
    "    if os.path.exists(\"data/articles_preprocessed.pkl\"):\n",
    "        df_articles = pd.read_pickle(\"data/articles_preprocessed.pkl\")\n",
    "    else:\n",
    "        df_articles = concatenate_data()\n",
    "        df_articles[\"text\"] = preprocess(df_articles[\"text\"])\n",
    "        # drop all rows where \"text\" is only whitespace or empty\n",
    "        df_articles = df_articles[df_articles[\"text\"].str.strip().astype(bool)]\n",
    "        df_articles.to_csv(\"data/articles_preprocessed.csv\", index=False)\n",
    "        df_articles.to_pickle(\"data/articles_preprocessed.pkl\")\n",
    "    return df_articles\n",
    "\n",
    "def load_or_create_tfidf(df_articles):\n",
    "    print(\"Loading or creating TF-IDF data...\")\n",
    "    # use tfidf data file if it exists, else create it\n",
    "    if os.path.exists(\"data/articles_tfidf.pkl\"):\n",
    "        df_articles_tfidf = pd.read_pickle(\"data/articles_tfidf.pkl\")\n",
    "    else:\n",
    "        # creating a new TF-IDF matrix\n",
    "        tfidf = TfidfVectorizer(stop_words=\"english\", strip_accents=\"unicode\", min_df=MIN_DF)\n",
    "        tfidf = TfidfVectorizer(max_features=MAX_FEATURES, strip_accents=\"unicode\", min_df=MIN_DF, max_df=MAX_DF)\n",
    "        tfidf_article_array = tfidf.fit_transform(df_articles[\"text\"])\n",
    "        df_articles_tfidf = pd.DataFrame(tfidf_article_array.toarray(), index=df_articles.index, columns=tfidf.get_feature_names_out())\n",
    "        df_articles_tfidf.to_csv(\"data/articles_tfidf.csv\", index=False)\n",
    "        df_articles_tfidf.to_pickle(\"data/articles_tfidf.pkl\")\n",
    "    return df_articles_tfidf\n",
    "\n",
    "def load_or_create_pca(df_articles_tfidf):\n",
    "    print(\"Loading or creating PCA data...\")\n",
    "    # use pca data file if it exists, else create it\n",
    "    if os.path.exists(\"data/articles_pca.pkl\"):\n",
    "        df_articles_pca = pd.read_pickle(\"data/articles_pca.pkl\")\n",
    "    else:\n",
    "        # using PCA to reduce the dimensionality\n",
    "        scaler = MinMaxScaler()\n",
    "        data_rescaled = scaler.fit_transform(df_articles_tfidf)\n",
    "        # variance explained by 90% of components\n",
    "        pca = PCA(n_components = 0.90)\n",
    "        pca.fit(data_rescaled)\n",
    "        reduced = pca.transform(data_rescaled)\n",
    "        df_articles_pca = pd.DataFrame(data=reduced)\n",
    "        df_articles_pca.to_pickle(\"data/articles_pca.pkl\")\n",
    "        # Calculate the variance explained by principle components\n",
    "        print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "        print(' Number of components:', pca.n_components_)\n",
    "    return df_articles_pca\n",
    "\n",
    "def print_metrics(df_articles, df_articles_labeled):\n",
    "    # print metrics\n",
    "    print(f\"Adjusted Mutual Information: {adjusted_mutual_info_score(df_articles['news_group'], df_articles_labeled['cluster'])}\")\n",
    "    print(f\"Adjusted Rand Score: {adjusted_rand_score(df_articles['news_group'], df_articles_labeled['cluster'])}\")\n",
    "    print(f\"Completeness Score: {completeness_score(df_articles['news_group'], df_articles_labeled['cluster'])}\")\n",
    "    print()\n",
    "\n",
    "def findOptimalEps(n_neighbors, data):\n",
    "    '''\n",
    "    function to find optimal eps distance when using DBSCAN; based on this article: https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\n",
    "    '''\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nbrs = neigh.fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.plot(distances)\n",
    "    plt.show()\n",
    "\n",
    "def cluster_using_kmeans(df_articles, df_for_prediction, ncluster):\n",
    "    # using KMeans clustering\n",
    "    print(f\"Clustering using KMeans with {ncluster} clusters\")\n",
    "    kmeans = KMeans(n_clusters=ncluster, n_init='auto')\n",
    "    articles_predictions = kmeans.fit_predict(df_for_prediction)\n",
    "    df_articles_predicted = df_articles.copy()\n",
    "    df_articles_predicted[\"cluster\"] = articles_predictions\n",
    "    # TODO: save model\n",
    "    joblib.dump(kmeans, MODEL_PATH+'kmeans_n' +str(ncluster)+'.joblib')\n",
    "    return df_articles_predicted\n",
    "\n",
    "def cluster_using_whc(df_articles, df_for_prediction, ncluster):\n",
    "    # using Ward Hierarchical Clustering\n",
    "    print(f\"Clustering using Ward Hierarchical Clustering with {ncluster} clusters\")\n",
    "    whc = AgglomerativeClustering(n_clusters=ncluster, linkage=\"ward\")\n",
    "    articles_predictions = whc.fit_predict(df_for_prediction)\n",
    "    df_articles_predicted = df_articles.copy()\n",
    "    df_articles_predicted[\"cluster\"] = articles_predictions\n",
    "    # TODO: save model\n",
    "    joblib.dump(whc, MODEL_PATH+'whc_n' +str(ncluster)+'.joblib')\n",
    "    return df_articles_predicted\n",
    "\n",
    "def cluster_using_ac(df_articles, df_for_prediction, ncluster):\n",
    "    # using Agglomerative Clustering\n",
    "    print(f\"Clustering using Agglomerative Clustering with {ncluster} clusters\")\n",
    "    ac = AgglomerativeClustering(n_clusters=ncluster, linkage=\"average\")\n",
    "    articles_predictions = ac.fit_predict(df_for_prediction)\n",
    "    df_articles_predicted = df_articles.copy()\n",
    "    df_articles_predicted[\"cluster\"] = articles_predictions\n",
    "    # TODO: save model\n",
    "    joblib.dump(ac, MODEL_PATH+'ac_n' +str(ncluster)+'.joblib')\n",
    "    return df_articles_predicted\n",
    "\n",
    "def my_silhouette_score(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    return score\n",
    "\n",
    "def cluster_using_dbscan(df_articles, df_for_prediction, ncluster):\n",
    "    # using DBSCAN clustering\n",
    "    print(f\"Clustering using DBSCAN with {ncluster} clusters\")\n",
    "    # findOptimalEps(2, df_for_prediction)\n",
    "    dbscan = DBSCAN()\n",
    "    param_grid = {\n",
    "    'eps': [0.5, 1.0, 1.5],\n",
    "    'min_samples': [5, 10, 15],\n",
    "    'metric': ['euclidean', 'manhattan'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 30, 50]\n",
    "    }\n",
    "    grid = GridSearchCV(dbscan, param_grid, cv=5, verbose=0, n_jobs=-1,scoring=my_silhouette_score)\n",
    "    grid.fit(df_for_prediction)\n",
    "    best_estimator = grid.best_estimator_\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "    print(\"Best score:\", grid.best_score_)\n",
    "    articles_predictions = best_estimator.predict(df_for_prediction)\n",
    "    df_articles_predicted = df_articles.copy()\n",
    "    df_articles_predicted[\"cluster\"] = articles_predictions\n",
    "    # TODO: save model\n",
    "    joblib.dump(dbscan, MODEL_PATH+'dbscan_n' +str(ncluster)+'.joblib')\n",
    "    return df_articles_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading or creating preprocessed data...\n",
      "Loading or creating TF-IDF data...\n",
      "Clustering using DBSCAN with 10 clusters\n"
     ]
    }
   ],
   "source": [
    "df_articles = load_or_create_preprocessed()\n",
    "df_articles_tfidf = load_or_create_tfidf(df_articles)\n",
    "# df_articles_pca = load_or_create_pca(df_articles_tfidf)\n",
    "ncluster=10\n",
    "df_for_prediction = df_articles_tfidf\n",
    "dbscan = cluster_using_dbscan(df_articles, df_for_prediction, ncluster)\n",
    "print_metrics(df_articles, dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
